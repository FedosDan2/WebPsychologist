import json
import os
import hashlib
from typing import List, Dict, Any
os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
from llama_cpp import Llama
import torch
from transformers import pipeline
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np


class RAGPsychologyAdvisor:
    def __init__(self, knowledge_base_path: str = "psychology_knowledge_base.json"):
        # === 1. –ó–∞–≥—Ä—É–∂–∞–µ–º LLM ===
        print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º LLM (Saiga Mistral 7B GGUF)...")
        self.llm = Llama(
            model_path="/home/fedosdan2/prog/pr_act/PROJECT/backend/model/mistral/saiga_mistral_7b.Q4_K_M.gguf",
            n_ctx=2048,
            n_threads=6,  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ CPU-–ø–æ—Ç–æ–∫–æ–≤
            verbose=False
        )

        # === 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π ===
        print(f"üìö –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π: {knowledge_base_path}")
        with open(knowledge_base_path, "r", encoding="utf-8") as f:
            self.knowledge_base = json.load(f)

        # === 3. –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥-–º–æ–¥–µ–ª—å (–Ω–∞ CPU) ===
        print("üß† –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥-–º–æ–¥–µ–ª—å...")
        self.embedding_model = SentenceTransformer('intfloat/multilingual-e5-large', device='cpu')

        # === 4. –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º FAISS-–∏–Ω–¥–µ–∫—Å —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º ===
        self._load_or_build_index(knowledge_base_path)

        print("‚úÖ RAG-—Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")

    def _compute_file_hash(self, filepath: str) -> str:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π."""
        hash_md5 = hashlib.md5()
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def _load_or_build_index(self, knowledge_base_path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –∏–∑ –∫—ç—à–∞ –∏–ª–∏ —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–π."""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏ –∫ –∫—ç—à-—Ñ–∞–π–ª–∞–º
        cache_dir = "rag_cache"
        os.makedirs(cache_dir, exist_ok=True)
        base_name = os.path.splitext(os.path.basename(knowledge_base_path))[0]
        index_path = os.path.join(cache_dir, f"{base_name}.faiss")
        texts_path = os.path.join(cache_dir, f"{base_name}_texts.json")
        hash_path = os.path.join(cache_dir, f"{base_name}.hash")

        # –í—ã—á–∏—Å–ª—è–µ–º —Ç–µ–∫—É—â–∏–π —Ö–µ—à –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        current_hash = self._compute_file_hash(knowledge_base_path)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –∫—ç—à
        if (os.path.exists(index_path) and
            os.path.exists(texts_path) and
            os.path.exists(hash_path)):

            with open(hash_path, "r") as f:
                cached_hash = f.read().strip()
            if cached_hash == current_hash:
                print("üîç –ó–∞–≥—Ä—É–∂–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π FAISS-–∏–Ω–¥–µ–∫—Å...")
                self.index = faiss.read_index(index_path)
                with open(texts_path, "r", encoding="utf-8") as f:
                    self.kb_texts = json.load(f)
                return

        # –ï—Å–ª–∏ –∫—ç—à –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ —É—Å—Ç–∞—Ä–µ–ª ‚Äî —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π
        print("üîç –ö—ç—à –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ —É—Å—Ç–∞—Ä–µ–ª. –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å...")
        self._build_faiss_index()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å, —Ç–µ–∫—Å—Ç—ã –∏ —Ö–µ—à
        faiss.write_index(self.index, index_path)
        with open(texts_path, "w", encoding="utf-8") as f:
            json.dump(self.kb_texts, f, ensure_ascii=False)
        with open(hash_path, "w") as f:
            f.write(current_hash)

    def _build_faiss_index(self):
        """–°–æ–∑–¥–∞—ë—Ç FAISS-–∏–Ω–¥–µ–∫—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π."""
        self.kb_texts = []
        for item in self.knowledge_base:
            text = " ".join(item.get("keywords", [])) + " " + item["content"]
            self.kb_texts.append(text)

        print("  ‚Üí –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        embeddings = self.embedding_model.encode(
            self.kb_texts,
            convert_to_numpy=True,
            normalize_embeddings=True
        ).astype('float32')

        dim = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dim)
        self.index.add(embeddings)
        print(f"  ‚Üí –ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω. –í–µ–∫—Ç–æ—Ä–æ–≤: {self.index.ntotal}")

    def _retrieve_relevant_facts(self, query: str, top_k: int = 3) -> List[Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç top_k —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ü–∏—Ç–∞—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π."""
        query_emb = self.embedding_model.encode(
            query,
            convert_to_numpy=True,
            normalize_embeddings=True
        ).astype('float32')
        query_emb = np.expand_dims(query_emb, axis=0)

        distances, indices = self.index.search(query_emb, top_k)

        results = []
        for idx in indices[0]:
            if idx < len(self.knowledge_base):
                results.append(self.knowledge_base[idx])
        return results

    def _build_prompt(self, analysis: Dict[str, Any], retrieved_facts: List[Dict]) -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM."""
        summary_lines = []
        summary_lines.append(f"–î–∏–∞–ª–æ–≥: {analysis.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
        summary_lines.append(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {analysis.get('total_messages_analyzed', 0)}")

        dominant = [f"{t['topic']} ({t['percentage']}%)" for t in analysis.get('dominant_topics', [])]
        if dominant:
            summary_lines.append(f"–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã: {', '.join(dominant)}")

        participants_info = []
        for name, data in analysis.get('participants_analysis', {}).items():
            emotion = data.get('dominant_emotion', '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')
            disc_raw = data.get('text_dominant', '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω')
            if isinstance(disc_raw, list):
                disc_str = "/".join([str(x) for x in disc_raw if isinstance(x, str)])
            elif isinstance(disc_raw, str):
                disc_str = disc_raw
            else:
                disc_str = "–Ω–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω"
            main_topic = data.get('topic_interests', {}).get('main_interest', '–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞')
            participants_info.append(f"- {name}: –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∞—è —ç–º–æ—Ü–∏—è ‚Äî {emotion}, —Å—Ç–∏–ª—å DISC ‚Äî {disc_str}, –∏–Ω—Ç–µ—Ä–µ—Å—ã ‚Äî {main_topic}")
        
        if participants_info:
            summary_lines.append("–£—á–∞—Å—Ç–Ω–∏–∫–∏:\n" + "\n".join(participants_info))

        analysis_summary = "\n".join(summary_lines)

        facts_text = "\n".join([
            f"‚Ä¢ {item['content']} (–ò—Å—Ç–æ—á–Ω–∏–∫: {item['source']})"
            for item in retrieved_facts
        ]) or "–ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."

        prompt = f"""–¢—ã ‚Äî –ª–∏—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥ —Å 15-–ª–µ—Ç–Ω–∏–º —Å—Ç–∞–∂–µ–º. –ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –ø–µ—Ä–µ–ø–∏—Å–∫–∏ –∏ –Ω–∞—É—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–∞–π –∫—Ä–∞—Ç–∫–∏–π, –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–π –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π —Å–æ–≤–µ—Ç.

–ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–ø–∏—Å–∫–∏:
{analysis_summary}

–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
{facts_text}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –î–∞–≤–∞–π 3-5 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–æ–≤–µ—Ç–∞.
- –°—Å—ã–ª–∞—ë—à—å—Å—è –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: ¬´–ö–∞–∫ –æ—Ç–º–µ—á–∞–µ—Ç –î–∂. –ì–æ—Ç—Ç–º–∞–Ω‚Ä¶¬ª, ¬´–°–æ–≥–ª–∞—Å–Ω–æ –º–æ–¥–µ–ª–∏ DISC‚Ä¶¬ª.
- –ò–∑–±–µ–≥–∞–π –æ–±—â–∏—Ö —Ñ—Ä–∞–∑ –≤—Ä–æ–¥–µ ¬´–Ω—É–∂–Ω–æ –ª—É—á—à–µ –æ–±—â–∞—Ç—å—Å—è¬ª.
- –ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –≤ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–µ–º, –Ω–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–º —Ç–æ–Ω–µ.
- –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ –¥–ª–∏–Ω–Ω–µ–µ 500 —Å–ª–æ–≤.

–û—Ç–≤–µ—Ç:"""
        return prompt

    def generate_advice(self, analysis: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–æ–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∏ RAG."""
        topics = [t["topic"] for t in analysis.get("dominant_topics", [])]
        emotions = list({data.get("dominant_emotion") for data in analysis.get("participants_analysis", {}).values() if data.get("dominant_emotion")})
        
        disc_tokens = set()
        for data in analysis.get("participants_analysis", {}).values():
            for key in ["text_dominant", "test_dominant"]:
                val = data.get(key)
                if isinstance(val, list):
                    for v in val:
                        if isinstance(v, str):
                            disc_tokens.add(v)
                elif isinstance(val, str):
                    disc_tokens.add(val)
        disc_list = sorted(disc_tokens)

        query = f"{' '.join(topics)} {' '.join(emotions)} {' '.join(disc_list)}"
        query = query.strip() or "–æ–±—â–∏–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –º–µ–∂–ª–∏—á–Ω–æ—Å—Ç–Ω–æ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏"

        retrieved = self._retrieve_relevant_facts(query, top_k=3)
        prompt = self._build_prompt(analysis, retrieved)

        try:
            output = self.llm(
                prompt,
                max_tokens=256,       # ‚Üê –Ω–µ –±–æ–ª—å—à–µ 128!
                temperature=0.7,
                stop=["–ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–ø–∏—Å–∫–∏:", "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:", "\n\n"],
                echo=False
            )
            return output["choices"][0]["text"].strip()
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}"