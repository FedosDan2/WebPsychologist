from transformers import AutoTokenizer, AutoModelForCausalLM
import torch, json


# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
_model = None
_tokenizer = None

def get_phi3_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç Phi-3-mini –æ–¥–∏–Ω —Ä–∞–∑ –∏ –∫—ç—à–∏—Ä—É–µ—Ç."""
    global _model, _tokenizer
    if _model is None:
        print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º Phi-3-mini –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏...")
        model_id = "microsoft/Phi-3-mini-4k-instruct"

        _tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            trust_remote_code=True
        )
        _model = AutoModelForCausalLM.from_pretrained(
            model_id,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True,
            load_in_4bit=True  # —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏
        )
        _model.eval()
    return _model, _tokenizer

def interpret_analysis(analysis_data, max_new_tokens=256):
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é.
    
    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        analysis_data (dict): —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ analyze_dialog_by_participant
        max_new_tokens (int): –º–∞–∫—Å. –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        str: —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π –≤—ã–≤–æ–¥
    """
    model, tokenizer = get_phi3_model()

    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —Å —á—ë—Ç–∫–∏–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏
    prompt = f"""–¢—ã ‚Äî —ç—Ç–∏—á–Ω—ã–π –∏ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –ª–∏—á–Ω—ã—Ö –ø–µ—Ä–µ–ø–∏—Å–æ–∫. 
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –¥–∞—Ç—å –¥–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–π –∏ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥.
–ù–µ –≤—ã–¥—É–º—ã–≤–∞–π, –Ω–µ —Å—Ç–∞–≤—å –¥–∏–∞–≥–Ω–æ–∑—ã, –Ω–µ –æ—Ü–µ–Ω–∏–≤–∞–π –ª–∏—á–Ω–æ—Å—Ç—å. –ì–æ–≤–æ—Ä–∏ —Ç–æ–ª—å–∫–æ –æ –ø–æ–≤–µ–¥–µ–Ω–∏–∏ –≤ —ç—Ç–æ–º –¥–∏–∞–ª–æ–≥–µ.
–ò–∑–±–µ–≥–∞–π –æ–±—â–∏—Ö —Ñ—Ä–∞–∑ –≤—Ä–æ–¥–µ ¬´–æ–±—â–∞–ª–∏—Å—å –Ω–∞ —Ä–∞–∑–Ω—ã–µ —Ç–µ–º—ã¬ª. –î–µ–ª–∞–π –∞–∫—Ü–µ–Ω—Ç –Ω–∞ —ç–º–æ—Ü–∏—è—Ö –∏ —Ç–µ–º–∞—Ö.

–ú–µ—Ç—Ä–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞:
{json.dumps(analysis_data, ensure_ascii=False, indent=2)}

–í—ã–≤–æ–¥ (–Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ):"""

    messages = [{"role": "user", "content": prompt}]

    input_ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            input_ids,
            max_new_tokens=max_new_tokens,
            do_sample=False,  # –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            pad_token_id=tokenizer.eos_token_id
        )

    response = outputs[0][input_ids.shape[-1]:]
    result = tokenizer.decode(response, skip_special_tokens=True).strip()
    return result