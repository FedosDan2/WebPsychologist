from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch

class LightRussianLLM:
    def __init__(self, model_size="tiny"):
        """
        model_size –≤–∞—Ä–∏–∞–Ω—Ç—ã:
        - 'tiny': 1.1 GB (TinyLlama) - —Å–∞–º—ã–π –ª—ë–≥–∫–∏–π
        - 'small': 3 GB (mGPT) - –±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–æ/—Ä–∞–∑–º–µ—Ä
        - 'medium': 7 GB (Saiga 7B) - –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
        """
        
        model_map = {
            'tiny': "ai-forever/TinyLlama-1.1B-Chat-v1.0",      # 1.1 GB
            'small': "ai-forever/mGPT",                         # 3 GB
            'medium': "IlyaGusev/saiga_mistral_7b_gguf"         # 4 GB
        }
        
        self.model_name = model_map.get(model_size, model_map['tiny'])
        
        print(f" –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å: {self.model_name}")
        print(f" –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {self.get_model_size(model_size)}")
        
        try:
            self.pipe = pipeline(
                "text-generation",
                model=self.model_name,
                torch_dtype=torch.float16,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–æ–≤–∏–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å
                device_map="auto",          # –ê–≤—Ç–æ–≤—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
                model_kwargs={"load_in_8bit": True} if model_size == 'medium' else {}
            )
            print(" –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        except Exception as e:
            print(f" –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            print(" –ü—Ä–æ–±—É—é –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–µ–∑ GPU...")
            self.pipe = pipeline(
                "text-generation",
                model=self.model_name,
                device=-1  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ CPU
            )
    
    def get_model_size(self, size):
        sizes = {
            'tiny': "1.1 GB (—Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –ª—é–±–æ–º –ü–ö)",
            'small': "3 GB (–Ω—É–∂–Ω–æ 8+ GB RAM)", 
            'medium': "4 GB (–Ω—É–∂–Ω–æ 16+ GB RAM)"
        }
        return sizes.get(size, "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
    
    def generate_psych_advice(self, analysis_text):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"""
        
        prompt = f"""
–¢—ã –ø—Å–∏—Ö–æ–ª–æ–≥-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ –∏ –¥–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:

{analysis_text}

–î–∞–π 2-3 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–æ–≤–µ—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—â–µ–Ω–∏—è.
–ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º.
"""
        
        try:
            result = self.pipe(
                prompt,
                max_new_tokens=150,  # –ö–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                temperature=0.7,
                do_sample=True
            )
            return result[0]['generated_text']
        except Exception as e:
            return f" –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}\nüí° –ü–æ–ø—Ä–æ–±—É–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å model_size='tiny'"

# –¢–µ—Å—Ç —Å–∞–º–æ–π –ª—ë–≥–∫–æ–π –º–æ–¥–µ–ª–∏
if __name__ == "__main__":
    print("üß™ –¢–µ—Å—Ç —Å–∞–º–æ–π –ª—ë–≥–∫–æ–π —Ä—É—Å—Å–∫–æ–π –º–æ–¥–µ–ª–∏...")
    
    # –ü–æ–ø—Ä–æ–±—É–π —Å–Ω–∞—á–∞–ª–∞ tiny, –µ—Å–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç - small
    llm = LightRussianLLM(model_size="tiny")
    
    test_analysis = """
    –ê–ª–µ–∫—Å–µ–π: D-—Ç–∏–ø (—Ä–µ—à–∏—Ç–µ–ª—å–Ω—ã–π), –≥–æ–≤–æ—Ä–∏—Ç –±—ã—Å—Ç—Ä–æ, –ø—Ä—è–º–æ–ª–∏–Ω–µ–π–Ω–æ.
    –ò—Ä–∏–Ω–∞: S-—Ç–∏–ø (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π), –≥–æ–≤–æ—Ä–∏—Ç –º—è–≥–∫–æ, —á–∞—Å—Ç–æ –∏–∑–≤–∏–Ω—è–µ—Ç—Å—è.
    """
    
    advice = llm.generate_psych_advice(test_analysis)
    print("\n –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –û–¢ –õ–Å–ì–ö–û–ô –ú–û–î–ï–õ–ò:")
    print("-" * 40)
    print(advice)
    print("-" * 40)